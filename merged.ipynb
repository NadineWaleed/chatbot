{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNI8+CIEKCbd5dM9mkrsv4T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NadineWaleed/chatbot/blob/main/merged.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers faiss-cpu --quiet\n",
        "!pip install langchain langchain-community langchain-core chromadb groq pypdf mistralai"
      ],
      "metadata": {
        "id": "pcKQHlGOdrmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77CDvjmwdqbp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import faiss\n",
        "import numpy as np\n",
        "from groq import Groq\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from mistralai import Mistral\n",
        "from mistralai import DocumentURLChunk, ImageURLChunk, TextChunk\n",
        "from mistralai.models import OCRResponse\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = Mistral(api_key=\"1H5nc5Yx3SH9nPTttAav1QgudKvM81WJ\")\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "groq_client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "print(\"Groq API initialized.\")\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "api_token = \"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\"\n",
        "me = api.whoami(token=api_token)\n",
        "print(me)"
      ],
      "metadata": {
        "id": "d_Qq08XwfDTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = Path(\"/content/AllDocs\")\n",
        "pdf_files = list(folder_path.glob(\"*.pdf\"))\n",
        "assert pdf_files, \"No PDF files found in the specified folder.\"\n"
      ],
      "metadata": {
        "id": "sPkFHBbmec0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_files"
      ],
      "metadata": {
        "id": "AbOUj2e1e_eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_markdowns = []\n",
        "\n",
        "for pdf_file in pdf_files:\n",
        "    print(f\"Processing: {pdf_file.name}\")\n",
        "\n",
        "    uploaded_file = client.files.upload(\n",
        "        file={\n",
        "            \"file_name\": pdf_file.stem,\n",
        "            \"content\": pdf_file.read_bytes(),\n",
        "        },\n",
        "        purpose=\"ocr\",\n",
        "    )\n",
        "\n",
        "    signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
        "\n",
        "    pdf_response = client.ocr.process(\n",
        "        document=DocumentURLChunk(document_url=signed_url.url),\n",
        "        model=\"mistral-ocr-latest\",\n",
        "    )\n",
        "\n",
        "    response_dict = json.loads(pdf_response.model_dump_json())\n",
        "    json_string = json.dumps(response_dict, indent=4)\n",
        "    with open('ocr_output.json', 'w') as f:\n",
        "        f.write(json_string)\n",
        "\n",
        "    print(\"OCR processing complete\")\n",
        "    for page in response_dict['pages']:\n",
        "        content = page['markdown']\n",
        "        content = re.sub(r'^(#+)\\s*', r'\\1 ', content, flags=re.MULTILINE)\n",
        "        all_markdowns.append(content)\n",
        "\n",
        "\n",
        "combined_markdown = \"\\n\\n\".join(all_markdowns)\n",
        "with open('combined_markdown.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(combined_markdown)\n",
        "\n",
        "print(\"‚úÖ All PDFs processed and combined into markdown.\")\n"
      ],
      "metadata": {
        "id": "QPcs6YEVfBo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"combined_markdown.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    document_text = f.read()\n"
      ],
      "metadata": {
        "id": "IM-ik-Ztg80o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Setup ===\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import faiss\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from groq import Groq\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# === Config ===\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "groq_client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "embedder = SentenceTransformer(\"intfloat/e5-small-v2\")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# === Load and preprocess document ===\n",
        "with open(\"combined_markdown.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    document_text = f.read()\n",
        "\n",
        "# === Chunk by headers ===\n",
        "def chunk_text_with_headers(text, chunk_size=500, overlap=50):\n",
        "    chunks = []\n",
        "    current_header = \"Unknown\"\n",
        "    buffer = []\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "    for line in lines:\n",
        "        if re.match(r'^#+\\s+.*', line.strip()):\n",
        "            current_header = line.strip().replace(\"#\", \"\").strip()\n",
        "        buffer.append(line)\n",
        "\n",
        "        if len(\" \".join(buffer).split()) >= chunk_size:\n",
        "            chunk = \"\\n\".join(buffer).strip()\n",
        "            chunks.append({\"text\": chunk, \"header\": current_header})\n",
        "            buffer = buffer[-overlap:]\n",
        "\n",
        "    if buffer:\n",
        "        chunks.append({\"text\": \"\\n\".join(buffer).strip(), \"header\": current_header})\n",
        "\n",
        "    return chunks\n",
        "\n",
        "headered_chunks = chunk_text_with_headers(document_text)\n",
        "\n",
        "# === Embedding ===\n",
        "def embed_chunks(chunks):\n",
        "    chunk_texts = [f\"passage: {c['text']}\" for c in chunks]\n",
        "    embeddings = embedder.encode(chunk_texts, show_progress_bar=True)\n",
        "    return np.array(embeddings), chunk_texts, [c['header'] for c in chunks]\n",
        "\n",
        "def build_faiss_index(embeddings):\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "embeddings, chunk_texts, headers = embed_chunks(headered_chunks)\n",
        "index = build_faiss_index(embeddings)\n",
        "\n",
        "# === Smart Prompt Handling ===\n",
        "def extract_section_from_query(query):\n",
        "    match = re.search(r\"(?:about|of|regarding|on)\\s+(.*?components)\", query.lower())\n",
        "    if match:\n",
        "        return match.group(1).strip().title()\n",
        "    return \"Components\"\n",
        "\n",
        "def is_structural_component_question(query: str) -> bool:\n",
        "    query = query.lower()\n",
        "    return bool(re.search(r\"(what|list).*(components|modules|parts|features).*of\", query))\n",
        "\n",
        "def build_prompt(query, context):\n",
        "    if is_structural_component_question(query):\n",
        "        section_name = extract_section_from_query(query)\n",
        "        return f\"\"\"\n",
        "You are given structured text extracted from a document. It may contain headings and bullet points.\n",
        "\n",
        "Your task is to extract **all detailed information related to the section titled \\\"{section_name}\\\"**.\n",
        "This includes any sub-points or bullet entries listed under it.\n",
        "\n",
        "Be exhaustive:\n",
        "- Maintain structure and indentation\n",
        "- Include every bullet, sub-point, and associated description\n",
        "- Add brief explanations if available in context\n",
        "\n",
        "Return a complete list using bullet points or numbered lists.\n",
        "\n",
        "Only use the provided context. Do not make up information.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\"\"\".strip()\n",
        "    else:\n",
        "        return f\"\"\"\n",
        "You are a precise assistant. Use ONLY the context below to answer the question in a complete and well-explained way.\n",
        "\n",
        "Rules:\n",
        "- Include all relevant points.\n",
        "- Use bullet points or paragraphs for structure.\n",
        "- Do not make up any information.\n",
        "- Be clear and detailed.\n",
        "- Use clean formatting.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\"\"\".strip()\n",
        "\n",
        "# === Token Safe Querying ===\n",
        "def truncate_text(text, max_words=400):\n",
        "    words = text.split()\n",
        "    return \" \".join(words[:max_words])\n",
        "\n",
        "def count_tokens(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def query_groq(prompt: str, model: str = \"llama3-70b-8192\", temperature: float = 0.3, max_tokens: int = 4000) -> str:\n",
        "    response = groq_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a precise and helpful assistant. Only answer using the context provided.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def clean_response(response):\n",
        "    response = re.sub(r'#+\\s+', '', response)\n",
        "    response = re.sub(r'[-*]\\s+', '‚Ä¢ ', response)\n",
        "    lines = response.split('\\n')\n",
        "    for i in range(len(lines)):\n",
        "        if re.match(r'^\\d+\\.\\s+', lines[i]):\n",
        "            lines[i] = re.sub(r'^\\d+\\.\\s+', lambda m: f\"{m.group(0)}\", lines[i])\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "# === Final RAG Function ===\n",
        "def ask_question_with_groq(query: str, index, chunk_texts, headers, top_k: int = 3, max_tokens_allowed: int = 5500):\n",
        "    query_embedding = embedder.encode(f\"query: {query}\")\n",
        "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
        "\n",
        "    context_chunks = []\n",
        "    total_tokens = 0\n",
        "\n",
        "    for i in indices[0]:\n",
        "        chunk = truncate_text(chunk_texts[i])\n",
        "        tokens = count_tokens(chunk)\n",
        "        if total_tokens + tokens > max_tokens_allowed:\n",
        "            break\n",
        "        context_chunks.append(chunk)\n",
        "        total_tokens += tokens\n",
        "\n",
        "    context = \"\\n\\n\".join(context_chunks)\n",
        "    prompt = build_prompt(query, context)\n",
        "    raw_response = query_groq(prompt)\n",
        "    cleaned = clean_response(raw_response)\n",
        "\n",
        "    sources = \"\\n\\nSources:\\n\" + \"\\n\".join([\n",
        "        f\"- Chunk {i+1}: {headers[idx]}\" for i, idx in enumerate(indices[0][:len(context_chunks)])\n",
        "    ])\n",
        "\n",
        "    return cleaned + sources\n",
        "\n",
        "# === Example Usage ===\n",
        "if __name__ == \"__main__\":\n",
        "    questions = [\n",
        "        \"What are the IQVIA components and their functions?\",\n",
        "        \"What are the GAHAC components?\",\n",
        "        \"Explain POS\",\n",
        "        \"State to me all that has ASCII File\",\n",
        "        \"What are the business rules for adding new family?\",\n",
        "        \"What Main challenges do we face in Universal health coverage?\"\n",
        "    ]\n",
        "    for q in questions:\n",
        "        print(f\"\\nüîç Question: {q}\")\n",
        "        print(ask_question_with_groq(q, index, chunk_texts, headers))\n"
      ],
      "metadata": {
        "id": "LD_w15_07if3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ask_question_with_groq(\"State to me all that has ASCII File\", index, chunk_texts, headers))"
      ],
      "metadata": {
        "id": "UnjBd6-H_DI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Setup ===\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import faiss\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from groq import Groq\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# === Config ===\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "groq_client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "embedder = SentenceTransformer(\"intfloat/e5-small-v2\")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# === Load and preprocess document ===\n",
        "with open(\"combined_markdown.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    document_text = f.read()\n",
        "\n",
        "# === Chunk by headers ===\n",
        "def chunk_text_with_headers(text, chunk_size=300, overlap=100):\n",
        "    chunks = []\n",
        "    current_header = \"Unknown\"\n",
        "    buffer = []\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "    for line in lines:\n",
        "        if re.match(r'^#+\\s+.*', line.strip()):\n",
        "            current_header = line.strip().replace(\"#\", \"\").strip()\n",
        "        buffer.append(line)\n",
        "\n",
        "        if len(\" \".join(buffer).split()) >= chunk_size:\n",
        "            chunk = \"\\n\".join(buffer).strip()\n",
        "            chunks.append({\"text\": chunk, \"header\": current_header})\n",
        "            buffer = buffer[-overlap:]\n",
        "\n",
        "    if buffer:\n",
        "        chunks.append({\"text\": \"\\n\".join(buffer).strip(), \"header\": current_header})\n",
        "\n",
        "    return chunks\n",
        "\n",
        "headered_chunks = chunk_text_with_headers(document_text)\n",
        "\n",
        "# === Embedding ===\n",
        "def embed_chunks(chunks):\n",
        "    chunk_texts = [f\"passage: {c['text']}\" for c in chunks]\n",
        "    embeddings = embedder.encode(chunk_texts, show_progress_bar=True)\n",
        "    return np.array(embeddings), chunk_texts, [c['header'] for c in chunks]\n",
        "\n",
        "def build_faiss_index(embeddings):\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "embeddings, chunk_texts, headers = embed_chunks(headered_chunks)\n",
        "index = build_faiss_index(embeddings)\n",
        "\n",
        "# === Smart Prompt Handling ===\n",
        "def extract_section_from_query(query):\n",
        "    match = re.search(r\"(?:about|of|regarding|on)\\s+(.*?components)\", query.lower())\n",
        "    if match:\n",
        "        return match.group(1).strip().title()\n",
        "    return \"Components\"\n",
        "\n",
        "def is_structural_component_question(query: str) -> bool:\n",
        "    query = query.lower()\n",
        "    return bool(re.search(r\"(what|list).*(components|modules|parts|features).*of\", query))\n",
        "\n",
        "def build_prompt(query, context):\n",
        "    if is_structural_component_question(query):\n",
        "        section_name = extract_section_from_query(query)\n",
        "        return f\"\"\"\n",
        "You are given structured text extracted from a document. It may contain headings and bullet points.\n",
        "\n",
        "Your task is to extract **all detailed information related to the section titled \\\"{section_name}\\\"**.\n",
        "This includes any sub-points or bullet entries listed under it.\n",
        "\n",
        "Be exhaustive:\n",
        "- Maintain structure and indentation\n",
        "- Include every bullet, sub-point, and associated description\n",
        "- Add brief explanations if available in context\n",
        "\n",
        "Return a complete list using bullet points or numbered lists.\n",
        "\n",
        "Only use the provided context. Do not make up information.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\"\"\".strip()\n",
        "    else:\n",
        "        return f\"\"\"\n",
        "You are a precise assistant. Use ONLY the context below to answer the question in a complete and well-explained way.\n",
        "\n",
        "Rules:\n",
        "- Include all relevant points.\n",
        "- Use bullet points or paragraphs for structure.\n",
        "- Do not make up any information.\n",
        "- Be clear and detailed.\n",
        "- Use clean formatting.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\"\"\".strip()\n",
        "\n",
        "# === Token Safe Querying ===\n",
        "def truncate_text(text, max_words=400):\n",
        "    words = text.split()\n",
        "    return \" \".join(words[:max_words])\n",
        "\n",
        "def count_tokens(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def query_groq(prompt: str, model: str = \"llama3-70b-8192\", temperature: float = 0.3, max_tokens: int = 4000) -> str:\n",
        "    response = groq_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a precise and helpful assistant. Only answer using the context provided.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def clean_response(response):\n",
        "    response = re.sub(r'#+\\s+', '', response)\n",
        "    response = re.sub(r'[-*]\\s+', '‚Ä¢ ', response)\n",
        "    lines = response.split('\\n')\n",
        "    for i in range(len(lines)):\n",
        "        if re.match(r'^\\d+\\.\\s+', lines[i]):\n",
        "            lines[i] = re.sub(r'^\\d+\\.\\s+', lambda m: f\"{m.group(0)}\", lines[i])\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "# === Final RAG Function ===\n",
        "def ask_question_with_groq(query: str, index, chunk_texts, headers, top_k: int = 8, max_tokens_allowed: int = 5500):\n",
        "    query_embedding = embedder.encode(f\"query: {query}\")\n",
        "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
        "\n",
        "    context_chunks = []\n",
        "    total_tokens = 0\n",
        "\n",
        "    for i in indices[0]:\n",
        "        chunk = truncate_text(chunk_texts[i])\n",
        "        tokens = count_tokens(chunk)\n",
        "        if total_tokens + tokens > max_tokens_allowed:\n",
        "            break\n",
        "        context_chunks.append(chunk)\n",
        "        total_tokens += tokens\n",
        "\n",
        "    context = \"\\n\\n\".join(context_chunks)\n",
        "    prompt = build_prompt(query, context)\n",
        "    raw_response = query_groq(prompt)\n",
        "    cleaned = clean_response(raw_response)\n",
        "\n",
        "\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "# === Example Usage ===\n",
        "if __name__ == \"__main__\":\n",
        "    questions = [\n",
        "        \"What are the IQVIA components and their functions?\",\n",
        "        \"What are the GAHAC components?\",\n",
        "        \"Explain POS\",\n",
        "        \"State to me all that has ASCII File\",\n",
        "        \"What are the business rules for adding new family?\",\n",
        "        \"What Main challenges do we face in Universal health coverage?\"\n",
        "    ]\n",
        "    for q in questions:\n",
        "        print(f\"\\nüîç Question: {q}\")\n",
        "        print(ask_question_with_groq(q, index, chunk_texts, headers))\n"
      ],
      "metadata": {
        "id": "yT1nsKmg_IB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import faiss\n",
        "from pathlib import Path\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from groq import Groq\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "groq_client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "embedder = SentenceTransformer(\"intfloat/e5-small-v2\")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "with open(\"combined_markdown.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    document_text = f.read()\n",
        "\n",
        "def chunk_text_with_headers(text, chunk_size=300, overlap=100):\n",
        "    chunks = []\n",
        "    current_header = \"Unknown\"\n",
        "    buffer = []\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "    for line in lines:\n",
        "        if re.match(r'^#+\\s+.*', line.strip()):\n",
        "            current_header = line.strip().replace(\"#\", \"\").strip()\n",
        "        buffer.append(line)\n",
        "\n",
        "        if len(\" \".join(buffer).split()) >= chunk_size:\n",
        "            chunk = \"\\n\".join(buffer).strip()\n",
        "            chunks.append({\"text\": chunk, \"header\": current_header})\n",
        "            buffer = buffer[-overlap:]\n",
        "\n",
        "    if buffer:\n",
        "        chunks.append({\"text\": \"\\n\".join(buffer).strip(), \"header\": current_header})\n",
        "\n",
        "    return chunks\n",
        "\n",
        "headered_chunks = chunk_text_with_headers(document_text)\n",
        "\n",
        "def embed_chunks(chunks):\n",
        "    chunk_texts = [f\"passage: {c['text']}\" for c in chunks]\n",
        "    embeddings = embedder.encode(chunk_texts, show_progress_bar=True)\n",
        "    return np.array(embeddings), chunk_texts, [c['header'] for c in chunks]\n",
        "\n",
        "def build_faiss_index(embeddings):\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "embeddings, chunk_texts, headers = embed_chunks(headered_chunks)\n",
        "index = build_faiss_index(embeddings)\n",
        "\n",
        "def extract_section_from_query(query):\n",
        "    match = re.search(r\"(?:about|of|regarding|on)\\s+(.*?components)\", query.lower())\n",
        "    if match:\n",
        "        return match.group(1).strip().title()\n",
        "    return \"Components\"\n",
        "\n",
        "def is_structural_component_question(query: str) -> bool:\n",
        "    query = query.lower()\n",
        "    return bool(re.search(r\"(what|list).*(components|modules|parts|features).*of\", query))\n",
        "\n",
        "def build_prompt(query, context):\n",
        "    if is_structural_component_question(query):\n",
        "        section_name = extract_section_from_query(query)\n",
        "        return f\"\"\"\n",
        "You are given structured text extracted from a document. It may contain headings and bullet points.\n",
        "\n",
        "Your task is to extract **all detailed information related to the section titled \\\"{section_name}\\\"**.\n",
        "This includes any sub-points or bullet entries listed under it.\n",
        "\n",
        "Be exhaustive:\n",
        "- Maintain structure and indentation\n",
        "- Include every bullet, sub-point, and associated description\n",
        "- Add brief explanations if available in context\n",
        "\n",
        "Return a complete list using bullet points or numbered lists.\n",
        "\n",
        "Only use the provided context. Do not make up information.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\"\"\".strip()\n",
        "    else:\n",
        "        return f\"\"\"\n",
        "You are a precise assistant. Use ONLY the context below to answer the question in a complete and well-explained way.\n",
        "\n",
        "Rules:\n",
        "- Include all relevant points.\n",
        "- Use bullet points or paragraphs for structure.\n",
        "- Do not make up any information.\n",
        "- Be clear and detailed.\n",
        "- Use clean formatting.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\"\"\".strip()\n",
        "\n",
        "def truncate_text(text, max_words=400):\n",
        "    words = text.split()\n",
        "    return \" \".join(words[:max_words])\n",
        "\n",
        "def count_tokens(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def query_groq(prompt: str, model: str = \"llama3-70b-8192\", temperature: float = 0.3, max_tokens: int = 4000) -> str:\n",
        "    response = groq_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a precise and helpful assistant. Only answer using the context provided.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def clean_response(response):\n",
        "    response = re.sub(r'#+\\s+', '', response)\n",
        "    response = re.sub(r'[-*]\\s+', '‚Ä¢ ', response)\n",
        "    lines = response.split('\\n')\n",
        "    for i in range(len(lines)):\n",
        "        if re.match(r'^\\d+\\.\\s+', lines[i]):\n",
        "            lines[i] = re.sub(r'^\\d+\\.\\s+', lambda m: f\"{m.group(0)}\", lines[i])\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def ask_question_with_groq(query: str, index, chunk_texts, headers, top_k: int = 8, max_tokens_allowed: int = 5500):\n",
        "    query_embedding = embedder.encode(f\"query: {query}\")\n",
        "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
        "\n",
        "    context_chunks = []\n",
        "    total_tokens = 0\n",
        "\n",
        "    for i in indices[0]:\n",
        "        chunk = truncate_text(chunk_texts[i])\n",
        "        tokens = count_tokens(chunk)\n",
        "        if total_tokens + tokens > max_tokens_allowed:\n",
        "            break\n",
        "        context_chunks.append(chunk)\n",
        "        total_tokens += tokens\n",
        "\n",
        "    context = \"\\n\\n\".join(context_chunks)\n",
        "    prompt = build_prompt(query, context)\n",
        "    raw_response = query_groq(prompt)\n",
        "    cleaned = clean_response(raw_response)\n",
        "\n",
        "\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "# === Example Usage ===\n",
        "if __name__ == \"__main__\":\n",
        "    questions = [\n",
        "        \"What are the IQVIA components and their functions?\",\n",
        "        \"What are the GAHAC components?\",\n",
        "        \"Explain POS\",\n",
        "        \"State to me all that has ASCII File\",\n",
        "        \"What are the business rules for adding new family?\",\n",
        "        \"What Main challenges do we face in Universal health coverage?\"\n",
        "    ]\n",
        "    for q in questions:\n",
        "        print(f\"\\nüîç Question: {q}\")\n",
        "        print(ask_question_with_groq(q, index, chunk_texts, headers))\n"
      ],
      "metadata": {
        "id": "POLLzVmjKtLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ntdOndfTUSxR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}