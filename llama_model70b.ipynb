{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6ryTq6Y4Qel"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers faiss-cpu --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community langchain-core chromadb groq pypdf mistralai"
      ],
      "metadata": {
        "id": "MvqG-_si4m-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "from groq import Groq\n",
        "from mistralai import Mistral\n",
        "from mistralai import DocumentURLChunk, ImageURLChunk, TextChunk\n",
        "from mistralai.models import OCRResponse\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "OpokarZ-4aKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = Path(\"/content/AllDocs\")\n",
        "pdf_files = list(folder_path.glob(\"*.pdf\"))\n",
        "assert pdf_files, \"No PDF files found in the specified folder.\"\n"
      ],
      "metadata": {
        "id": "BPXut7peJAdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_files"
      ],
      "metadata": {
        "id": "V09W4vvBMfAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Mistral(api_key=\"1H5nc5Yx3SH9nPTttAav1QgudKvM81WJ\")\n"
      ],
      "metadata": {
        "id": "p5W9Gm64Msa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_markdowns = []\n",
        "\n",
        "for pdf_file in pdf_files:\n",
        "    print(f\"Processing: {pdf_file.name}\")\n",
        "\n",
        "    uploaded_file = client.files.upload(\n",
        "        file={\n",
        "            \"file_name\": pdf_file.stem,\n",
        "            \"content\": pdf_file.read_bytes(),\n",
        "        },\n",
        "        purpose=\"ocr\",\n",
        "    )\n",
        "\n",
        "    signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
        "\n",
        "    pdf_response = client.ocr.process(\n",
        "        document=DocumentURLChunk(document_url=signed_url.url),\n",
        "        model=\"mistral-ocr-latest\",\n",
        "    )\n",
        "\n",
        "    response_dict = json.loads(pdf_response.model_dump_json())\n",
        "\n",
        "    for page in response_dict['pages']:\n",
        "        content = page['markdown']\n",
        "        content = re.sub(r'^(#+)\\s*', r'\\1 ', content, flags=re.MULTILINE)\n",
        "        all_markdowns.append(content)\n"
      ],
      "metadata": {
        "id": "tB-AvidSMh6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_markdown = \"\\n\\n\".join(all_markdowns)\n",
        "\n",
        "with open('combined_markdown.txt', 'w') as f:\n",
        "    f.write(combined_markdown)\n",
        "\n",
        "print(\"All PDFs processed and combined into markdown.\")\n"
      ],
      "metadata": {
        "id": "HA8qdiFyM0X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "import json\n",
        "from mistralai import Mistral\n",
        "from mistralai import DocumentURLChunk\n",
        "\n",
        "client = Mistral(api_key=\"1H5nc5Yx3SH9nPTttAav1QgudKvM81WJ\")\n",
        "\n",
        "folder_path = Path(\"/content/AllDocs\")\n",
        "pdf_files = list(folder_path.glob(\"*.pdf\"))\n",
        "assert pdf_files, \"No PDF files found in the specified folder.\"\n",
        "\n",
        "all_markdowns = []\n",
        "\n",
        "for pdf_file in pdf_files:\n",
        "    print(f\"Processing: {pdf_file.name}\")\n",
        "\n",
        "    uploaded_file = client.files.upload(\n",
        "        file={\n",
        "            \"file_name\": pdf_file.stem,\n",
        "            \"content\": pdf_file.read_bytes(),\n",
        "        },\n",
        "        purpose=\"ocr\",\n",
        "    )\n",
        "\n",
        "    signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
        "    pdf_response = client.ocr.process(\n",
        "        document=DocumentURLChunk(document_url=signed_url.url),\n",
        "        model=\"mistral-ocr-latest\",\n",
        "        include_image_base64=True\n",
        "    )\n",
        "    response_dict = json.loads(pdf_response.model_dump_json())\n",
        "\n",
        "    for page in response_dict['pages']:\n",
        "        content = page['markdown']\n",
        "        content = re.sub(r'^(#+)\\s*', r'\\1 ', content, flags=re.MULTILINE)\n",
        "        all_markdowns.append(content)\n",
        "\n",
        "combined_markdown = \"\\n\\n\".join(all_markdowns)\n",
        "with open('combined_markdown.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(combined_markdown)\n",
        "\n",
        "print(\"âœ… All PDFs processed and combined into markdown.\")\n"
      ],
      "metadata": {
        "id": "oPGncf6ntg80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "# import re\n",
        "\n",
        "# folder_path = Path(\"/content/AllDocs\")\n",
        "# pdf_files = list(folder_path.glob(\"*.pdf\"))\n",
        "# assert pdf_files, \"No PDF files found in the specified folder.\"\n",
        "# all_markdowns = []\n",
        "\n",
        "# for pdf_file in pdf_files:\n",
        "#     print(f\"Processing: {pdf_file.name}\")\n",
        "\n",
        "#     uploaded_file = client.files.upload(\n",
        "#         file={\n",
        "#             \"file_name\": pdf_file.stem,\n",
        "#             \"content\": pdf_file.read_bytes(),\n",
        "#         },\n",
        "#         purpose=\"ocr\",\n",
        "#     )\n",
        "\n",
        "#     signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
        "\n",
        "#     pdf_response = client.ocr.process(\n",
        "#         document=DocumentURLChunk(document_url=signed_url.url),\n",
        "#         model=\"mistral-ocr-latest\",\n",
        "#     )\n",
        "\n",
        "#     # Optionally, save raw OCR response to a JSON file for debugging\n",
        "#     with open(f\"{pdf_file.stem}_ocr_output.json\", 'w') as f:\n",
        "#         json.dump(pdf_response.model_dump_json(), f, indent=4)\n",
        "\n",
        "#     # Parse the OCR response\n",
        "#     response_dict = json.loads(pdf_response.model_dump_json())\n",
        "\n",
        "#     for page in response_dict['pages']:\n",
        "#         content = page['markdown']\n",
        "#         content = re.sub(r'^(#+)\\s*', r'\\1 ', content, flags=re.MULTILINE)\n",
        "#         all_markdowns.append(content)\n",
        "\n",
        "# combined_markdown = \"\\n\\n\".join(all_markdowns)\n",
        "\n",
        "# with open('combined_markdown.txt', 'w') as f:\n",
        "#     f.write(combined_markdown)\n",
        "\n",
        "# print(\"All PDFs processed and combined into markdown.\")\n"
      ],
      "metadata": {
        "id": "JOHkiVB6uQHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "processing a file"
      ],
      "metadata": {
        "id": "5IGo3992qFOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# client = Mistral(api_key=\"1H5nc5Yx3SH9nPTttAav1QgudKvM81WJ\")\n",
        "# pdf_file = Path(\"/content/Universal Health Insurance Project Architecture (1) (1).pdf\")\n",
        "# assert pdf_file.is_file()\n",
        "# print(\"Uploading PDF...\")\n",
        "# uploaded_file = client.files.upload(\n",
        "#     file={\n",
        "#         \"file_name\": pdf_file.stem,\n",
        "#         \"content\": pdf_file.read_bytes(),\n",
        "#     },\n",
        "#     purpose=\"ocr\",\n",
        "# )\n",
        "\n",
        "# signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
        "# print(\"Processing OCR...\")\n",
        "# pdf_response = client.ocr.process(\n",
        "#     document=DocumentURLChunk(document_url=signed_url.url),\n",
        "#     model=\"mistral-ocr-latest\",\n",
        "#     include_image_base64=True\n",
        "# )\n",
        "# response_dict = json.loads(pdf_response.model_dump_json())\n",
        "# json_string = json.dumps(response_dict, indent=4)\n",
        "# with open('ocr_output.json', 'w') as f:\n",
        "#     f.write(json_string)\n",
        "\n",
        "# print(\"OCR processing complete\")"
      ],
      "metadata": {
        "id": "EubccSsj4wmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('ocr_output.json', 'r') as f:\n",
        "#     ocr_response = json.load(f)\n",
        "\n",
        "# markdowns = []\n",
        "# for page in ocr_response['pages']:\n",
        "#     content = page['markdown']\n",
        "#     content = re.sub(r'^(#+)\\s*', r'\\1 ', content, flags=re.MULTILINE)\n",
        "#     markdowns.append(content)\n",
        "# combined_markdown = \"\\n\\n\".join(markdowns)\n",
        "# with open('combined_markdown.txt', 'w') as f:\n",
        "#     f.write(combined_markdown)\n",
        "\n",
        "# print(\"Text processing complete\")"
      ],
      "metadata": {
        "id": "M25BA85w4w-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"combined_markdown.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    document_text = f.read()\n"
      ],
      "metadata": {
        "id": "4e1KE5dU4_lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size=500, overlap=150):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = \" \".join(words[i:i+chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(document_text)\n",
        "print(f\"Total chunks: {len(chunks)}\")\n"
      ],
      "metadata": {
        "id": "a7zSN-B05D44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def chunk_by_heading(text):\n",
        "    lines = text.splitlines()\n",
        "    chunks = []\n",
        "    buffer = \"\"\n",
        "\n",
        "    for line in lines:\n",
        "        if re.match(r\"^[A-Z][A-Za-z0-9\\s&-]{2,}$\", line.strip()):\n",
        "            if buffer:\n",
        "                chunks.append(buffer.strip())\n",
        "                buffer = \"\"\n",
        "        buffer += line + \"\\n\"\n",
        "\n",
        "    if buffer:\n",
        "        chunks.append(buffer.strip())\n",
        "\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "EKxCpGVzLTwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def chunk_text_with_headers(text, chunk_size=300, overlap=50):\n",
        "#     chunks = []\n",
        "#     current_header = \"Unknown\"\n",
        "#     buffer = []\n",
        "\n",
        "#     lines = text.split('\\n')\n",
        "#     for line in lines:\n",
        "#         # Detect headers (simple heuristic)\n",
        "#         if re.match(r'^#+\\s+.*', line.strip()):\n",
        "#             current_header = line.strip().replace(\"#\", \"\").strip()\n",
        "#         buffer.append(line)\n",
        "\n",
        "#         # If buffer large enough, split\n",
        "#         if len(\" \".join(buffer).split()) >= chunk_size:\n",
        "#             chunk = \"\\n\".join(buffer).strip()\n",
        "#             chunks.append({\n",
        "#                 \"text\": chunk,\n",
        "#                 \"header\": current_header\n",
        "#             })\n",
        "#             buffer = buffer[-overlap:]  # retain overlap\n",
        "\n",
        "#     if buffer:\n",
        "#         chunks.append({\n",
        "#             \"text\": \"\\n\".join(buffer).strip(),\n",
        "#             \"header\": current_header\n",
        "#         })\n",
        "\n",
        "#     return chunks\n",
        "\n",
        "# headered_chunks = chunk_text_with_headers(document_text)\n",
        "# chunk_texts = [f\"passage: {c['text']}\" for c in headered_chunks]\n",
        "# chunk_embeddings = embedder.encode(chunk_texts, show_progress_bar=True)\n"
      ],
      "metadata": {
        "id": "OMiV4TTLUqwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "\n",
        "api_token = \"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\"\n",
        "me = api.whoami(token=api_token)\n",
        "print(me)\n"
      ],
      "metadata": {
        "id": "eED6M1hP5Krk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer(\n",
        "    \"intfloat/e5-small-v2\",\n",
        "    use_auth_token=\"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\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "9a7xfMRS5ha8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "embedder = SentenceTransformer(\"intfloat/e5-small-v2\")\n",
        "chunk_texts = [f\"passage: {chunk}\" for chunk in chunks]\n",
        "chunk_embeddings = embedder.encode(chunk_texts, show_progress_bar=True)\n",
        "dimension = chunk_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(np.array(chunk_embeddings))\n"
      ],
      "metadata": {
        "id": "j-3ah6gd5PHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "groq_client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "\n",
        "print(\"Groq API initialized.\")"
      ],
      "metadata": {
        "id": "VB-eDTRb5RZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "print(\"Memory buffer for chat history created.\")"
      ],
      "metadata": {
        "id": "LeZpGdor51gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_rag_prompt(context, question, chat_history=None):\n",
        "    \"\"\"\n",
        "    Creates a well-structured prompt for the RAG system that instructs\n",
        "    the model to only use provided context.\n",
        "    \"\"\"\n",
        "    system_prompt = \"\"\"\n",
        "    You are a precise and helpful assistant that only answers questions based on the provided context.\n",
        "\n",
        "    RULES:\n",
        "    1. Only use information from the provided context to answer the question.\n",
        "    2. If the answer cannot be found in the context, respond with: \"I cannot answer this question based on the provided documents.\"\n",
        "    3. Do not use prior knowledge to answer questions.\n",
        "    4. Keep your answers concise, clear, and well-structured.\n",
        "    5. When providing steps or processes, use a numbered list format.\n",
        "    6. For general explanations, use a clean paragraph structure.\n",
        "    7. Do not use markdown headings (##, ###) in your response.\n",
        "    8. Cite specific parts of the context when relevant by mentioning \"According to the document...\"\n",
        "    9. Never make up information that isn't present in the context.\n",
        "    10. Include ALL components, features, or items mentioned in the context.\n",
        "    11. Maintain the original structure and hierarchy of information.\n",
        "    12. If information appears in multiple documents, combine it comprehensively.\n",
        "    13. Do not summarize or omit details - provide complete information.\n",
        "    14. If there are multiple parts or components, list ALL of them.\n",
        "    15. Use clear formatting with bullet points and numbering.\n",
        "    16. Preserve relationships between components and their descriptions.\n",
        "    17. When asked about components or features mention them all.\n",
        "\n",
        "    FORMATTING:\n",
        "    - For steps or processes: Use numbered lists (1. Step one, 2. Step two)\n",
        "    - For key points: Use bullet points (â€¢ First point, â€¢ Second point)\n",
        "    - For explanations: Use clean paragraphs with clear topic sentences\n",
        "    - Highlight important terms using bold when appropriate\n",
        "    \"\"\"\n",
        "    chat_history_text = \"\"\n",
        "    if chat_history:\n",
        "        chat_history_text = \"\\n\\nChat History:\\n\"\n",
        "        for msg in chat_history:\n",
        "            role = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
        "            chat_history_text += f\"{role}: {msg['content']}\\n\"\n",
        "    full_prompt = f\"{system_prompt}\\n\\nCONTEXT:\\n{context}\\n{chat_history_text}\\n\\nQUESTION: {question}\\n\\nANSWER:\"\n",
        "\n",
        "    return full_prompt"
      ],
      "metadata": {
        "id": "mblMfv5o55yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_groq(prompt: str, model: str = \"llama3-70b-8192\", temperature: float = 0.3, max_tokens: int = 4000) -> str:\n",
        "    response = groq_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a precise and helpful assistant. Only answer using the context provided. \"\n",
        "                           \"If the context is insufficient, say you donâ€™t have enough information.\"\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "6VCGbplz59Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_response(response):\n",
        "    \"\"\"\n",
        "    Clean up the response to ensure consistent formatting, with special handling for equations\n",
        "    \"\"\"\n",
        "    response = re.sub(r'#+\\s+', '', response)\n",
        "    response = re.sub(r'[-*]\\s+', 'â€¢ ', response)\n",
        "    lines = response.split('\\n')\n",
        "    for i in range(len(lines)):\n",
        "        if re.match(r'^\\d+\\.\\s+', lines[i]):\n",
        "            lines[i] = re.sub(r'^\\d+\\.\\s+', lambda m: f\"{m.group(0)}\", lines[i])\n",
        "    response = re.sub(r'\\\\\\\\?\\[(.+?)\\\\\\\\?\\]', r'$$\\1$$', response)\n",
        "    if '\\\\[' in response and '\\\\]' not in response:\n",
        "        response = response.replace('\\\\[', '$$')\n",
        "        if not response.endswith('$$'):\n",
        "            response += '$$'\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "MQC0n8LM6Cep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "vZ0-eBud6FZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_rag(question, vectorstore, memory, k=8):\n",
        "    \"\"\"\n",
        "    Performs Retrieval-Augmented Generation with improved prompting and response formatting.\n",
        "\n",
        "    Args:\n",
        "        question: The user's question\n",
        "        vectorstore: The vector database containing document embeddings\n",
        "        memory: The conversation memory\n",
        "        k: Number of most relevant documents to retrieve\n",
        "\n",
        "    Returns:\n",
        "        A structured, clean answer based only on the retrieved context\n",
        "    \"\"\"\n",
        "    if not question or len(question.strip()) < 3:\n",
        "        return \"Please provide a more specific question.\"\n",
        "\n",
        "    relevant_docs = vectorstore.similarity_search(question, k=k)\n",
        "    if not relevant_docs:\n",
        "        return \"I cannot find any relevant information in the documents to answer your question.\"\n",
        "\n",
        "    context_parts = []\n",
        "    for i, doc in enumerate(relevant_docs):\n",
        "        header = doc.metadata.get('header', f'Section {i+1}')\n",
        "        context_parts.append(f\"DOCUMENT {i+1} ({header}):\\n{doc.page_content}\")\n",
        "\n",
        "    context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "    chat_history_raw = memory.load_memory_variables({}).get(\"chat_history\", [])\n",
        "\n",
        "    chat_history = []\n",
        "    for i in range(0, len(chat_history_raw), 2):\n",
        "        if i + 1 < len(chat_history_raw):\n",
        "            chat_history.append({\"role\": \"user\", \"content\": chat_history_raw[i].content})\n",
        "            chat_history.append({\"role\": \"assistant\", \"content\": chat_history_raw[i+1].content})\n",
        "\n",
        "    rag_prompt = create_rag_prompt(context, question, chat_history)\n",
        "    raw_response = query_groq(rag_prompt)\n",
        "    answer = clean_response(raw_response)\n",
        "    sources = \"\\n\\nSources:\\n\" + \"\\n\".join(\n",
        "        [f\"- Document {i+1}: {doc.metadata['header']}\"\n",
        "         for i, doc in enumerate(relevant_docs)]\n",
        "    )\n",
        "\n",
        "    final_answer = answer + sources\n",
        "\n",
        "    memory.save_context({\"input\": question}, {\"output\": final_answer})\n",
        "\n",
        "    return final_answer"
      ],
      "metadata": {
        "id": "4_bQ_BoJ6Ky7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_output(answer):\n",
        "    \"\"\"\n",
        "    Format the answer for display with clear structure and proper equation rendering\n",
        "    \"\"\"\n",
        "    contains_numbered_steps = bool(re.search(r'^\\d+\\.', answer, re.MULTILINE))\n",
        "    contains_equations = '\\\\' in answer or '$' in answer\n",
        "\n",
        "    if contains_numbered_steps:\n",
        "        formatted_answer = \"Process:\\n\" + answer\n",
        "    elif contains_equations:\n",
        "        formatted_answer = re.sub(r'([^\\$])\\$\\$(.*?)\\$\\$([^\\$])', r'\\1\\n$$\\2$$\\n\\3', formatted_answer)\n",
        "    else:\n",
        "        formatted_answer = answer\n",
        "\n",
        "    return formatted_answer"
      ],
      "metadata": {
        "id": "ivxtIzno6PT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def ask_question_with_groq(query: str, top_k: int = 3):\n",
        "#     query_embedding = embedder.encode(f\"query: {query}\")\n",
        "#     distances, indices = index.search(np.array([query_embedding]), top_k)\n",
        "\n",
        "#     context = \"\\n\\n\".join([chunks[i] for i in indices[0]])\n",
        "\n",
        "#     prompt = f\"\"\"Use the context below to answer the question.\n",
        "\n",
        "# Context:\n",
        "# {context}\n",
        "\n",
        "# Question:\n",
        "# {query}\n",
        "# \"\"\"\n",
        "#     return query_groq(prompt)\n"
      ],
      "metadata": {
        "id": "Pue_Dak8_HwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_question_with_groq(query: str, top_k: int = 3):\n",
        "    query_embedding = embedder.encode(f\"query: {query}\")\n",
        "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
        "\n",
        "    context = \"\\n\\n\".join([chunks[i] for i in indices[0]])\n",
        "    prompt = f\"\"\"Use the context below to answer the question.\n",
        "    Use the context below to answer the question in a detailed and complete manner.\n",
        "\n",
        "If multiple relevant details are present, list them all. Only use the provided context. Do not make up information.\n",
        "\n",
        "You are given structured text extracted from a document. It may contain headings and bullet points.\n",
        "Based on the context below, answer the question using bullet points to ensure all important information is included.\n",
        "Answer the question using only the context provided. Where relevant, include direct quotes from the context to support your answer.\n",
        "Read the context and answer the question with all available evidence. Be exhaustive â€” include all points, explanations, and any relevant details, even if they seem small.\n",
        "\n",
        "Only use the context. If something is unclear, say so.\n",
        "\n",
        "Do not add information that isn't in the context.\n",
        "\n",
        "Your task is to find and extract all detailed information related to the section titled . This includes sub-points or components listed below it.\n",
        "\n",
        "Be thorough and return a complete list of all the items mentioned under this section, maintaining any subheadings and bullet details if available.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\"\"\"\n",
        "    return query_groq(prompt)\n"
      ],
      "metadata": {
        "id": "YIwOLXXl0oQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "response = ask_question_with_groq(\"list to me al that has Microsoft SQL Server table\")\n",
        "print(\"Answer:\", response)\n"
      ],
      "metadata": {
        "id": "INTj4Htv6V93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "response = ask_question_with_groq(\"what are the business rules for adding new family?\")\n",
        "print(\"Answer:\", response)\n"
      ],
      "metadata": {
        "id": "ifrVWe6w65oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = ask_question_with_groq(\"explain POS\")\n",
        "print(\"Answer:\", response)\n"
      ],
      "metadata": {
        "id": "85_xWpKK_LUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = ask_question_with_groq(\"explain POS\")\n",
        "print(\"Answer:\", response)\n"
      ],
      "metadata": {
        "id": "mU5bxzRU0WYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = ask_question_with_groq(\"explain POS in details\")\n",
        "print(\"Answer:\", response)\n"
      ],
      "metadata": {
        "id": "OgOcEw_Iu2Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = ask_question_with_groq(\"what is all iqvia components?\")\n",
        "print(\"Answer:\", response)\n"
      ],
      "metadata": {
        "id": "RfwFNkST4hy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = ask_question_with_groq(\"what is Strengthening financial protection?\" )\n",
        "print(\"Answer:\", question)"
      ],
      "metadata": {
        "id": "YtRZh8ryQJUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "66XBO6rBSuNM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}