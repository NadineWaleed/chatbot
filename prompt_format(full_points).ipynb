{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPYEn9juRzvb0oTKUNrZZPv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NadineWaleed/chatbot/blob/main/prompt_format(full_points).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers faiss-cpu --quiet"
      ],
      "metadata": {
        "id": "MQtT3bqdn9mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community langchain-core chromadb groq pypdf mistralai"
      ],
      "metadata": {
        "id": "kd_Z3k01uC_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import faiss\n",
        "import numpy as np\n",
        "from groq import Groq\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "from groq import Groq\n",
        "from mistralai import Mistral\n",
        "from mistralai import DocumentURLChunk, ImageURLChunk, TextChunk\n",
        "from mistralai.models import OCRResponse\n",
        "from IPython.display import Markdown, display\n"
      ],
      "metadata": {
        "id": "izswX8jBukPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Mistral(api_key=\"1H5nc5Yx3SH9nPTttAav1QgudKvM81WJ\")\n"
      ],
      "metadata": {
        "id": "JlQ69GsTSXHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pdf_file = Path(\"/content/Universal Health Insurance Project Architecture (1) (1).pdf\")\n",
        "assert pdf_file.is_file()\n",
        "\n",
        "print(\"Uploading PDF...\")\n",
        "uploaded_file = client.files.upload(\n",
        "    file={\n",
        "        \"file_name\": pdf_file.stem,\n",
        "        \"content\": pdf_file.read_bytes(),\n",
        "    },\n",
        "    purpose=\"ocr\",\n",
        ")\n",
        "\n",
        "signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
        "\n",
        "print(\"Processing OCR...\")\n",
        "pdf_response = client.ocr.process(\n",
        "    document=DocumentURLChunk(document_url=signed_url.url),\n",
        "    model=\"mistral-ocr-latest\",\n",
        "    include_image_base64=True\n",
        ")\n",
        "\n",
        "response_dict = json.loads(pdf_response.model_dump_json())\n",
        "json_string = json.dumps(response_dict, indent=4)\n",
        "\n",
        "with open('ocr_output.json', 'w') as f:\n",
        "    f.write(json_string)\n",
        "\n",
        "print(\"OCR processing complete\")"
      ],
      "metadata": {
        "id": "zARt23g2vYWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('ocr_output.json', 'r') as f:\n",
        "    ocr_response = json.load(f)\n",
        "\n",
        "markdowns = []\n",
        "for page in ocr_response['pages']:\n",
        "    content = page['markdown']\n",
        "    content = re.sub(r'^(#+)\\s*', r'\\1 ', content, flags=re.MULTILINE)\n",
        "    markdowns.append(content)\n",
        "\n",
        "combined_markdown = \"\\n\\n\".join(markdowns)\n",
        "\n",
        "with open('combined_markdown.txt', 'w') as f:\n",
        "    f.write(combined_markdown)\n",
        "\n",
        "print(\"Text processing complete\")"
      ],
      "metadata": {
        "id": "AOLm1-sAvjNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"combined_markdown.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    document_text = f.read()\n"
      ],
      "metadata": {
        "id": "uRRqntO_vmbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MG-sDXsCvqDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "groq_client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "\n",
        "print(\"Groq API initialized.\")"
      ],
      "metadata": {
        "id": "9j3pSshNuhx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "\n",
        "api_token = \"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\"\n",
        "me = api.whoami(token=api_token)\n",
        "print(me)\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer(\n",
        "    \"intfloat/e5-small-v2\",\n",
        "    use_auth_token=\"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\"\n",
        ")\n",
        "\n",
        "\n",
        "embedder = SentenceTransformer(\"intfloat/e5-small-v2\")\n"
      ],
      "metadata": {
        "id": "9JaIb7Y4unK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# === Section Headers ===\n",
        "def chunk_text_with_headers(text, chunk_size=300, overlap=50):\n",
        "    chunks = []\n",
        "    current_header = \"Unknown\"\n",
        "    buffer = []\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "    for line in lines:\n",
        "        if re.match(r'^#+\\s+.*', line.strip()):\n",
        "            current_header = line.strip().replace(\"#\", \"\").strip()\n",
        "        buffer.append(line)\n",
        "\n",
        "        if len(\" \".join(buffer).split()) >= chunk_size:\n",
        "            chunk = \"\\n\".join(buffer).strip()\n",
        "            chunks.append({\n",
        "                \"text\": chunk,\n",
        "                \"header\": current_header\n",
        "            })\n",
        "            buffer = buffer[-overlap:]\n",
        "\n",
        "    if buffer:\n",
        "        chunks.append({\n",
        "            \"text\": \"\\n\".join(buffer).strip(),\n",
        "            \"header\": current_header\n",
        "        })\n",
        "\n",
        "    return chunks\n",
        "\n"
      ],
      "metadata": {
        "id": "PuIW8ISVuqxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding the chunks\n",
        "def embed_chunks(chunks):\n",
        "    chunk_texts = [f\"passage: {c['text']}\" for c in chunks]\n",
        "    embeddings = embedder.encode(chunk_texts, show_progress_bar=True)\n",
        "    return np.array(embeddings), chunk_texts, [c['header'] for c in chunks]\n",
        "\n",
        "def build_faiss_index(embeddings):\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n"
      ],
      "metadata": {
        "id": "GcDc4CbyutX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_groq(prompt: str, model: str = \"llama3-70b-8192\") -> str:\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        model=model,\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content.strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "-NAtYwDFuweA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "print(\"Memory buffer for chat history created.\")"
      ],
      "metadata": {
        "id": "rmqwkOM5v_EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_rag_prompt(context, question, chat_history=None):\n",
        "    \"\"\"\n",
        "    Creates a well-structured prompt for the RAG system that instructs\n",
        "    the model to only use provided context.\n",
        "    \"\"\"\n",
        "    system_prompt = \"\"\"\n",
        "    You are a precise and helpful assistant that only answers questions based on the provided context.\n",
        "\n",
        "    RULES:\n",
        "    1. Only use information from the provided context to answer the question.\n",
        "    2. If the answer cannot be found in the context, respond with: \"I cannot answer this question based on the provided documents.\"\n",
        "    3. Do not use prior knowledge to answer questions.\n",
        "    4. Keep your answers concise, clear, and well-structured.\n",
        "    5. When providing steps or processes, use a numbered list format.\n",
        "    6. For general explanations, use a clean paragraph structure.\n",
        "    7. Do not use markdown headings (##, ###,**) in your response.\n",
        "    8. Cite specific parts of the context when relevant by mentioning \"According to the document...\"\n",
        "    9. Never make up information that isn't present in the context.\n",
        "    10. Include ALL components, features, or items mentioned in the context.\n",
        "    11. Maintain the original structure and hierarchy of information.\n",
        "    12. If information appears in multiple documents, combine it comprehensively.\n",
        "    13. Do not summarize or omit details - provide complete information.\n",
        "    14. If there are multiple parts or components, list ALL of them.\n",
        "    15. Use clear formatting with bullet points and numbering.\n",
        "    16. Preserve relationships between components and their descriptions.\n",
        "\n",
        "    FORMATTING:\n",
        "    - For steps or processes: Use numbered lists (1. Step one, 2. Step two)\n",
        "    - For key points: Use bullet points (• First point, • Second point)\n",
        "    - For explanations: Use clean paragraphs with clear topic sentences\n",
        "    - Highlight important terms using bold when appropriate\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    chat_history_text = \"\"\n",
        "    if chat_history:\n",
        "        chat_history_text = \"\\n\\nChat History:\\n\"\n",
        "        for msg in chat_history:\n",
        "            role = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
        "            chat_history_text += f\"{role}: {msg['content']}\\n\"\n",
        "\n",
        "    full_prompt = f\"{system_prompt}\\n\\nCONTEXT:\\n{context}\\n{chat_history_text}\\n\\nQUESTION: {question}\\n\\nANSWER:\"\n",
        "\n",
        "    return full_prompt"
      ],
      "metadata": {
        "id": "PvwdfmVlv7yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "STSUQ8vIr2Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_groq(prompt: str, model: str = \"llama3-70b-8192\", temperature: float = 0.3, max_tokens: int = 4000) -> str:\n",
        "    response = groq_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a precise and helpful assistant. Only answer using the context provided. \"\n",
        "                           \"If the context is insufficient, say you don’t have enough information.\"\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "Jp0jJP6wwFu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_response(response):\n",
        "    \"\"\"\n",
        "    Clean up the response to ensure consistent formatting, with special handling for equations\n",
        "    \"\"\"\n",
        "    response = re.sub(r'#+\\s+', '', response)\n",
        "    response = re.sub(r'[-*]\\s+', '• ', response)\n",
        "    lines = response.split('\\n')\n",
        "    for i in range(len(lines)):\n",
        "        if re.match(r'^\\d+\\.\\s+', lines[i]):\n",
        "            lines[i] = re.sub(r'^\\d+\\.\\s+', lambda m: f\"{m.group(0)}\", lines[i])\n",
        "\n",
        "    response = re.sub(r'\\\\\\\\?\\[(.+?)\\\\\\\\?\\]', r'$$\\1$$', response)\n",
        "    response = re.sub(r'\\\\\\[([^\\]]+)$', r'$$\\1$$', response)\n",
        "    if '\\\\[' in response and '\\\\]' not in response:\n",
        "        response = response.replace('\\\\[', '$$')\n",
        "        if not response.endswith('$$'):\n",
        "            response += '$$'\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "LjbZA7avwHus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "54eKmdOawMBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_rag(question, vectorstore, memory, k=8):\n",
        "    \"\"\"\n",
        "    Performs Retrieval-Augmented Generation with improved prompting and response formatting.\n",
        "\n",
        "    Args:\n",
        "        question: The user's question\n",
        "        vectorstore: The vector database containing document embeddings\n",
        "        memory: The conversation memory\n",
        "        k: Number of most relevant documents to retrieve\n",
        "\n",
        "    Returns:\n",
        "        A structured, clean answer based only on the retrieved context\n",
        "    \"\"\"\n",
        "    if not question or len(question.strip()) < 3:\n",
        "        return \"Please provide a more specific question.\"\n",
        "    relevant_docs = vectorstore.similarity_search(question, k=k)\n",
        "    if not relevant_docs:\n",
        "        return \"I cannot find any relevant information in the documents to answer your question.\"\n",
        "    context_parts = []\n",
        "    for i, doc in enumerate(relevant_docs):\n",
        "        header = doc.metadata.get('header', f'Section {i+1}')\n",
        "        context_parts.append(f\"DOCUMENT {i+1} ({header}):\\n{doc.page_content}\")\n",
        "\n",
        "    context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "    chat_history_raw = memory.load_memory_variables({}).get(\"chat_history\", [])\n",
        "    chat_history = []\n",
        "    for i in range(0, len(chat_history_raw), 2):\n",
        "        if i + 1 < len(chat_history_raw):\n",
        "            chat_history.append({\"role\": \"user\", \"content\": chat_history_raw[i].content})\n",
        "            chat_history.append({\"role\": \"assistant\", \"content\": chat_history_raw[i+1].content})\n",
        "\n",
        "    rag_prompt = create_rag_prompt(context, question, chat_history)\n",
        "\n",
        "    raw_response = query_groq(rag_prompt)\n",
        "\n",
        "    answer = clean_response(raw_response)\n",
        "    sources = \"\\n\\nSources:\\n\" + \"\\n\".join(\n",
        "        [f\"- Document {i+1}: {doc.metadata['header']}\"\n",
        "         for i, doc in enumerate(relevant_docs)]\n",
        "    )\n",
        "\n",
        "    final_answer = answer + sources\n",
        "    memory.save_context({\"input\": question}, {\"output\": final_answer})\n",
        "\n",
        "    return final_answer"
      ],
      "metadata": {
        "id": "JE9UrmUJwRMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_output(answer):\n",
        "    \"\"\"\n",
        "    Format the answer for display with clear structure and proper equation rendering\n",
        "    \"\"\"\n",
        "    contains_numbered_steps = bool(re.search(r'^\\d+\\.', answer, re.MULTILINE))\n",
        "\n",
        "\n",
        "    if contains_numbered_steps:\n",
        "        formatted_answer = \"Process:\\n\" + answer\n",
        "    else:\n",
        "        formatted_answer = answer\n",
        "\n",
        "    return formatted_answer"
      ],
      "metadata": {
        "id": "6K2-eoePwUzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_section_prompt(query, context, section_name=\"Components\"):\n",
        "    return f\"\"\"\n",
        "You are given structured text extracted from a document. It may contain headings and bullet points.\n",
        "\n",
        "Your task is to find and extract **all detailed information related to the section titled \"{section_name}\"**. This includes any components or bullet points listed under it. Maintain structure and sub-bullets.\n",
        "\n",
        "Return a complete list with formatting.\n",
        "\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\"\"\".strip()\n",
        "def extract_section_from_query(query):\n",
        "    match = re.search(r\"(?:about|of|regarding|on)\\s+(.*?components)\", query.lower())\n",
        "    if match:\n",
        "        return match.group(1).strip().title()\n",
        "    return \"Components\"\n",
        "def ask_question_with_groq(query: str, index, chunk_texts, headers, top_k: int = 3):\n",
        "    query_embedding = embedder.encode(f\"query: {query}\")\n",
        "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
        "    context_chunks = [chunk_texts[i] for i in indices[0]]\n",
        "    section_name = extract_section_from_query(query)\n",
        "    context = \"\\n\\n\".join(context_chunks)\n",
        "    prompt = build_section_prompt(query, context, section_name)\n",
        "    return query_groq(prompt)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with open(\"combined_markdown.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        document_text = f.read()\n",
        "    headered_chunks = chunk_text_with_headers(document_text)\n",
        "    embeddings, chunk_texts, headers = embed_chunks(headered_chunks)\n",
        "    index = build_faiss_index(embeddings)\n",
        "    user_query = \"What are the IQVIA components and their functions?\"\n",
        "    answer = ask_question_with_groq(user_query, index, chunk_texts, headers)\n",
        "    print(\"\\n🔍 Answer:\\n\", answer)\n",
        "    user_query = \"What are the GAHAC components \"\n",
        "    answer = ask_question_with_groq(user_query, index, chunk_texts, headers)\n",
        "    print(\"\\n🔍 Answer:\\n\", answer)\n",
        "\n"
      ],
      "metadata": {
        "id": "CN8JOIEbn3N8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "        \"What are the IQVIA components and their functions?\",\n",
        "        \"What are the GAHAC components?\",\n",
        "        \"Explain POS\",\n",
        "        \"State to me all that has ASCII File\",\n",
        "        \"What are the business rules for adding new family?\",\n",
        "        \"What Main challenges do we face in Universal health coverage?\"\n",
        "    ]\n",
        "for q in questions:\n",
        "  # user_query = \"what is Point of Sale?\"\n",
        "  q = \"state to me all that has ASCII File\"\n",
        "  answer = ask_question_with_groq(q, index, chunk_texts, headers)\n",
        "  print(\"\\n🔍 Answer:\\n\", answer)\n",
        "\n"
      ],
      "metadata": {
        "id": "NfcmZBkcoOUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"What are the IQVIA components and their functions?\"\n",
        "answer = ask_question_with_groq(user_query, index, chunk_texts, headers)\n",
        "print(\"\\n🔍 Answer:\\n\", answer)\n",
        "user_query = \"What are the GAHAC components \"\n",
        "answer = ask_question_with_groq(user_query, index, chunk_texts, headers)\n",
        "print(\"\\n🔍 Answer:\\n\", answer)\n"
      ],
      "metadata": {
        "id": "OfzQIUOO21V9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}